---
title: "Project_2"
author: "Bhuvaneshwari Nattanmai Kuppusamy"
date: "3/17/2021"
output:
  pdf_document: default
  word_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r }
#install.packages("mlbench")
library(mlbench)
library(caret)
library(MASS)
library(tidyverse)
data("BreastCancer")
head(BreastCancer)
summary(BreastCancer)

```



```{r}
str(BreastCancer)
#Since Bare.nuclei has missing value,let us find the percentage of missing values to find out which method to implement to substitute missing values.
dim(BreastCancer)
number_rows <- nrow(BreastCancer)
number_rows
na_count <-sapply(BreastCancer, function(y) (sum(length(which(is.na(y))))/number_rows)*100)
na_count
paste0("Percentage of missing values in Bare.nuclei ",round(na_count[7],2), "%")

```
Data Description:

The BreastCancer data set has 699 observations/records, 10 predictor variables and 1 target variable.
Out of the 11 predictor variables,1- Character variable,9- Nominal or ordinal variable and 1- Target class

Also, it is found that there are only 2.29% of missing values in the variable Bare.nuclei.Hence, it is better delete the rows containing missing values


```{r }
#Deleting the rows with NA

BreastCancer.df <- na.omit(BreastCancer)

# The first variable "ID" will not make any sense in modeling phase. so,it is better remove it

BreastCancer.df$Id <- NULL


# Lets check our dataset

head(BreastCancer.df)

ind <- sample(2, nrow(BreastCancer.df), replace = TRUE, prob=c(0.8, 0.2))

```

#Splitting the dataset
```{r}
#install.packages("caTools")
library(caTools)
set.seed(1234)
split_ratio = sample.split(BreastCancer.df, SplitRatio = 0.7)
train = subset(BreastCancer.df, split_ratio==TRUE)
test = subset(BreastCancer.df, split_ratio==FALSE)
dim(BreastCancer.df)
print(dim(train)); print(dim(test))
names(test)[10] <- "Result"
test$Result <- as.factor(test$Result)

names(test)

names(train)[10] <- "Result"
train$Result <- as.factor(train$Result)

names(train)

```

Create multiple models using different classifiers/algorithms 

1. SVM

```{r}
#install.packages("e1071")
library(e1071)

# svm requires tuning
x.svm.tune <- tune(svm, Result~., data = train,
                   ranges = list(gamma = 2^(-8:1), cost = 2^(0:4)),
                   tunecontrol = tune.control(sampling = "fix")) 
# display the tuning results (in text format)
x.svm.tune #note the gamma and cost
# If the tuning results are on the margin of the parameters (e.g., gamma = 2^-8), 
# then widen the parameters.
x.svm <- svm(Result~., data = train, cost=1, gamma=0.00390625	, probability = TRUE) #

x.svm.pred <- predict(x.svm, type="class", newdata=test) #ensemble; only give the class
x.svm.prob <- predict(x.svm, type="prob", newdata=test, probability = TRUE) # has to include probability = TRUE while type="prob" is not needed
#t <- attr(x.svm.prob, "probabilities") # only give the probabilities
table(x.svm.pred,test$Result)


svm_accuracy <- round(((124 + 73) / nrow(test))*100,2)
paste0("The Accuracy of SVM model is ", svm_accuracy, "%")

```

2.Naive Bayes

```{r}
#install.packages("klaR")

library(klaR)
x.nb <- naiveBayes(Result ~ ., train, laplace = 0)
x.nb.pred <- predict(x.nb,test,type="class")
x.nb.prob <- predict(x.nb,test,type="raw")
table(x.nb.pred,test$Result)
nb_accuracy <- round(((125 + 75) / nrow(test))*100,2)
paste0("The Accuracy of NB model is ", nb_accuracy, "%")

```
3. Neural Network

```{r}
#install.packages("nnet")
library(nnet)
x.nnet <- nnet(Result ~ ., train, size=2)

x.nnet.pred <- predict(x.nnet,test,type="class")
x.nnet.prob <- predict(x.nnet,test,type="raw")
table(x.nnet.pred,test$Result)

neuralnet_accuracy <- round(((125 + 69) / nrow(test))*100,2)
paste0("The Accuracy of neuralnetwork model is ", neuralnet_accuracy, "%")

```
4. Decision Trees

```{r}
#install.packages("MASS")
library(MASS)
library(rpart)
library(rpart.plot)
x.rpart <- rpart(Result ~ ., train)
plot(x.rpart); text(x.rpart) 


prp(x.rpart, type = 1, extra = 1, split.font = 1, varlen = -10)  

#prediction
# predict classes for the evaluation data set
x.rpart.pred <- predict(x.rpart, type="class", newdata=test)  # to ensemble
# score the evaluation data set (extract the probabilities)
x.rpart.prob <- predict(x.rpart, type="prob", newdata=test)
table(x.rpart.pred,test$Result)
dtaccuracy <- round(((119 +70) / nrow(test))*100,2)
paste0("The Accuracy of Decision Trees model is ", dtaccuracy, "%")


```

5.conditional inference trees

```{r}
#install.packages("party")
library(party)
require(party)
x.ct <- ctree(Result ~ ., data=train)
plot(x.ct, main="Decision tree created using condition inference trees") 

x.ct.pred <- predict(x.ct, newdata=test) 
x.ct.prob <-  1- unlist(treeresponse(x.ct, test), use.names=F)[seq(1,nrow(test)*2,2)]
table(x.ct.pred,test$Result)
ctaccuracy <- round(((126 +71) / nrow(test))*100,2)
paste0("The Accuracy of condition inference tree model is ", ctaccuracy, "%")

```


6. Random Forests

```{r}
x.cf <- cforest(Result ~ ., train, control = cforest_unbiased(mtry = 9))

x.cf.pred <- predict(x.cf, newdata=test)
x.cf.prob <-  1- unlist(treeresponse(x.cf, test), use.names=F)[seq(1,nrow(test)*2,2)]

table(x.cf.pred, test$Result)

rfac <- round(((129 +71) / nrow(test))*100,2)
paste0("The Accuracy of Random Forest model is ", rfac, "%")


```


Leave-1-Out Cross Validation (LOOCV)


```{r}

ans <- numeric(length(BreastCancer.df[,1]))
for (i in 1:length(BreastCancer.df[,1])) {
  rp <- rpart(Class ~ ., BreastCancer.df[-i,])
  rp.predloo <- predict(rp,BreastCancer.df[i,],type="class")
  ans[i] <- rp.predloo
  }

ans <- as.factor(ans)
ans <- factor(ans, levels=c(1,2),
  labels=c('benign','malignant'))

ans <- factor(ans,labels=levels(BreastCancer.df$Class))

cm <- confusionMatrix(ans,BreastCancer.df$Class)
acc <- cm$overall['Accuracy']

paste0("The Accuracy of LOOCV model is ", acc, "%")



```

bagging (bootstrap aggregating)

```{r}
# create model using bagging (bootstrap aggregating)
require(ipred)
x.ip <- bagging(Result ~ ., data=train) 

x.ip.pred <- predict(x.ip, newdata=test)
x.ip.prob <- predict(x.ip, type="prob", newdata=test)
table(x.ip.pred,test$Result)
bagg_accuracy <- round(((124 +68) / nrow(test))*100,2)

paste0("The Accuracy of bagging model is ", bagg_accuracy, "%")
```


Quadratic Discriminant Analysis

```{r}
library(MASS)
library(dplyr)
train.num <- train %>% dplyr::select(-Result) %>% mutate_if(is.factor,as.character)%>% mutate_if(is.character,as.numeric)
train.num$Result <- train$Result
test.num <- test%>%dplyr::select(-Result) %>% mutate_if(is.factor,as.character)%>% mutate_if(is.character,as.numeric)
test.num$Result <- test$Result

x.qda <- qda(Result~., data = train.num) #qda, formula, right hand is non-factor
x.qda.pred <- predict(x.qda, test.num)$class
x.qda.prob <- predict(x.qda, test.num)$posterior 
table(x.qda.pred,test.num$Result)
qda_accuracy <- round(((121 +73) / nrow(test))*100,2)

paste0("The Accuracy of QDA model is ", qda_accuracy, "%")

```

Regularised Discriminant Analysis

```{r}
#not able to use test

library(klaR)
x.rda <- rda(Result~., data = train)
x.rda.pred <- predict(x.rda, test)$class
x.rda.prob <- predict(x.rda, test)$posterior
table(x.rda.pred,test$Result)
rda_accuracy <- round(((124 +74) / nrow(test))*100,2)

paste0("The Accuracy of RDA model is ", rda_accuracy, "%")

```

### Plot ROC curves to compare the performance of the individual classifiers.

```{r}
#load the ROCR package which draws the ROC curves
#install.packages("ROCR")
library(ROCR)


# 1.svm
x.svm.prob.rocr <- prediction(attr(x.svm.prob, "probabilities")[,2], test[,'Result'])
x.svm.perf <- performance(x.svm.prob.rocr, "tpr","fpr")

#2.nb
x.nb.prob.rocr <- prediction(x.nb.prob[,2], test[,'Result'])
x.nb.perf <- performance(x.nb.prob.rocr, "tpr","fpr")

#3.nnet
x.nn.prob.rocr <- prediction(x.nnet.prob, test[,'Result'])
x.nn.perf <- performance(x.nn.prob.rocr, "tpr","fpr")

#4. Decision Trees
x.rpart.prob.rocr <- prediction(x.rpart.prob[,2], test[,'Result'])
x.rpart.perf <- performance(x.rpart.prob.rocr, "tpr","fpr")

#5. conditional inference trees
x.ct.prob.rocr <- prediction(x.ct.prob, test[,'Result'])
x.ct.perf <- performance(x.ct.prob.rocr, "tpr","fpr")

#6. Random Forests
x.cf.prob.rocr <- prediction(x.cf.prob, test[,'Result'])
x.cf.perf <- performance(x.cf.prob.rocr, "tpr","fpr")

#7. bagging
x.ip.prob.rocr <- prediction(x.ip.prob[,2], test[,'Result'])
x.ip.perf <- performance(x.ip.prob.rocr, "tpr","fpr")

# 8.qda
x.qda.prob.rocr <- prediction(x.qda.prob[,2], test[,'Result'])
x.qda.perf <- performance(x.qda.prob.rocr, "tpr","fpr")

# 9.rda
x.rda.prob.rocr <- prediction(x.rda.prob[,2], test[,'Result'])
x.rda.perf <- performance(x.rda.prob.rocr, "tpr","fpr")
```


```{r}
####### plot
# Output the plot to a PNG file for display on web.  To draw to the screen, 
# comment this line out.
#png(filename="roc_curve_models1.png", width=700, height=700)

#par(mfrow=c(1,2))
plot(x.rpart.perf, col=2, main="ROC curves comparing classification performance \n of 9 machine learning models") # 
legend(0.6, 0.6, c('rpart', 'ctree', 'cforest','bagging','svm'), 2:6)# Draw a legend.
plot(x.ct.perf, col=3, add=TRUE)# add=TRUE draws on the existing chart  #has to be run together.
plot(x.cf.perf, col=4, add=TRUE)
plot(x.ip.perf, col=5, add=TRUE)
plot(x.svm.perf, col=6, add=TRUE)
# Close and save the PNG file.
#dev.off()

#png(filename="roc_curve_models2.png", width=700, height=700)
plot(x.nb.perf, col=7, main="ROC curves comparing classification performance \n of the other 4 machine learning models")
legend(0.6, 0.6, c('naive bayes', 'neural network', 'qda','rda'), 7:10)
plot(x.nn.perf, col=8, add=TRUE)
plot(x.qda.perf, col=9, add=TRUE)
plot(x.rda.perf, col=10, add=TRUE)
#dev.off()
```


Let us use  “majority rule” ensemble approach by stacking the previous algorithms svm, naive bayes, neural network, decision tree,Leave-1-Out Cross Validation, Regularised Discriminant Analysis and random forest. The overall accuracy of the ensemble model is 98.04% 

```{r}


combinedf <- data.frame(x.svm.pred,x.nb.pred,x.nnet.pred,x.rpart.pred,x.cf.pred,Class = test$Result, stringsAsFactors = F)


stvm <- svm(Class ~ ., combinedf)
stvm.pred <- predict(stvm, test)

table(stvm.pred,test$Result)
##            
## stvm.pred   benign malignant
##   benign       125         0
##   malignant      4        75
stack_accuracy <- round(((125 +75) / nrow(test))*100,2)

accuracy_df <- rbind("SVM Accuracy" = svm_accuracy, "Naive Bayes Accuracy" = nb_accuracy, "Neural Network Accuracy" = neuralnet_accuracy, "Decision Tree Accuracy" = dtaccuracy, "LOOCV Accuracy" = acc,  "RDA Accuracy" = rda_accuracy, "Random Forest Accuracy" = rfac, "Majority Ensemble Accuracy" = stack_accuracy)
accuracy_df

paste0("Therefore the overall ensemble majority model accuracy is ",stack_accuracy,"%")



```











